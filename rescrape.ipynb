{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml\n",
    "import requests\n",
    "import json\n",
    "from pathlib import Path\n",
    "import PyPDF2 as ppdf\n",
    "import string\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from ratelimit import limits, sleep_and_retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# define vars\n",
    "URL = \"https://e-dictionary.ilrdf.org.tw/wsReDictionary.htm\"\n",
    "\n",
    "original_dict = {\n",
    "    2: 'Amis',\n",
    "    6: 'Atayal',\n",
    "    24: 'Paiwan',\n",
    "    22: 'Bunun',\n",
    "    38: 'Puyma',\n",
    "    28: 'Rukai',\n",
    "    35: 'Tsou',\n",
    "    13: 'Saisiyat',\n",
    "    42: 'Yami',\n",
    "    14: 'Thao',\n",
    "    34: 'Kavalan',\n",
    "    33: 'Truku',\n",
    "    43: 'Sakizaya',\n",
    "    16: 'Seediq',\n",
    "    37: 'Saaroa',\n",
    "    36: 'Kanakanavu'\n",
    "}\n",
    "\n",
    "# Create a new dictionary with keys and values swapped\n",
    "TRIBES = {v: k for k, v in original_dict.items()}\n",
    "NAMES = sorted([\n",
    "    'Amis',\n",
    "    'Atayal',\n",
    "    'Paiwan',\n",
    "    'Bunun',\n",
    "    'Puyma',\n",
    "    'Rukai',\n",
    "    'Tsou',\n",
    "    'Saisiyat',\n",
    "    'Yami',\n",
    "    'Thao',\n",
    "    'Kavalan',\n",
    "    'Truku',\n",
    "    'Sakizaya',\n",
    "    'Seediq',\n",
    "    'Saaroa',\n",
    "    'Kanakanavu'\n",
    "])\n",
    "\n",
    "INTERVAL = 1000\n",
    "\n",
    "NAME_TO_IDX = {name: i for i, name in enumerate(NAMES)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# rate vars\n",
    "# this mostly shouldn't matter since API times are already slow-ish\n",
    "RATE_LIMIT = 25\n",
    "RATE_PERIOD = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def getWords(index: int) -> list: # scrape, do once -> provides wordlist\n",
    "    # Get path\n",
    "    folders = [folder for folder in os.listdir(os.getcwd()) if os.path.isdir(folder) and not folder[0] == \".\"]\n",
    "    folder = folders[index] # index\n",
    "    get_pdf = [file for file in os.listdir(folder)]\n",
    "    get_pdf = [file for file in get_pdf if re.search(r\".*\\.pdf\", file.lower())]\n",
    "    filepath = os.path.join(os.getcwd(), folder, get_pdf[0])\n",
    "\n",
    "    # Open it + scrape\n",
    "    all_tx = []\n",
    "    with open(filepath, 'rb') as f:\n",
    "        reader = ppdf.PdfReader(f)\n",
    "\n",
    "        for num in range(len(reader.pages)):\n",
    "            page = reader.pages[num]\n",
    "            all_tx.append(page.extract_text())\n",
    "\n",
    "    # Get words\n",
    "    fullstring = \"\"\n",
    "    for line in all_tx:\n",
    "        fullstring += line\n",
    "    \n",
    "    # Split + identify words\n",
    "    sep = fullstring.split(\"\\n\")\n",
    "    words = [word for word in sep if \"★\" in word]\n",
    "    phrase = r\"^([a-zA-Z'ʉ0-9\\s]+)\"\n",
    "    return [result.group().replace(\" \", \"\") for word in words if (result := re.search(phrase, word))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "@sleep_and_retry # if uncommented, ratelimiting\n",
    "@limits(calls=RATE_LIMIT, period=RATE_PERIOD)\n",
    "def getData(tribeName: str, qw: str) -> 'str or dict':\n",
    "    ask = {\n",
    "        \"FMT\": 1,\n",
    "        \"account\": \"E202403005\",\n",
    "        \"TribesCode\": TRIBES[tribeName],\n",
    "        \"qw\": qw\n",
    "    }\n",
    "\n",
    "    jsn_response = requests.post(URL, data=ask)\n",
    "    text = json.loads(jsn_response.text)\n",
    "    try:\n",
    "        assert jsn_response.status_code == 200\n",
    "        return text[\"GenericData\"]['DATA']\n",
    "    except:\n",
    "        return \"FAIL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def extractSentences(entry: dict) -> 'str or dict': # helper for processRequest\n",
    "    word = entry[\"Name\"]\n",
    "    check = entry['Explanation']\n",
    "    fr, zh = '', ''\n",
    "\n",
    "    try:\n",
    "        if isinstance(check, list):\n",
    "            check = check[0] # it works\n",
    "            if isinstance(check['Sentence'], dict):\n",
    "                fr = check['Sentence']['Original']\n",
    "                zh = check['Sentence']['Chinese']\n",
    "\n",
    "            elif isinstance(check['Sentence'], list):\n",
    "                fr = check['Sentence'][0]['Original']\n",
    "                zh = check['Sentence'][0]['Chinese']\n",
    "        \n",
    "        elif isinstance(check, dict):\n",
    "            if isinstance(check['Sentence'], dict):\n",
    "                fr = check['Sentence']['Original']\n",
    "                zh = check['Sentence']['Chinese']\n",
    "\n",
    "            elif isinstance(check['Sentence'], list):\n",
    "                fr = check['Sentence'][0]['Original']\n",
    "                zh = check['Sentence'][0]['Chinese']\n",
    "        return {word: (fr, zh)}\n",
    "    except:\n",
    "        return \"FAIL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def processRequest(response):  # response into dict of {word: (fr, zh)}\n",
    "    ret = {}\n",
    "    fails = []\n",
    "    if isinstance(response, list): # multiple entries\n",
    "        for i, entry in enumerate(response):\n",
    "            result = extractSentences(entry)\n",
    "            word = entry[\"Name\"]\n",
    "            if result == \"FAIL\":\n",
    "                print(word + \" has failed in extraction.\") # comment out if annoying \n",
    "                fails.append(word)\n",
    "            else:\n",
    "                ret.update(result)\n",
    "\n",
    "    elif isinstance(response, dict): # only 1 entry\n",
    "        result = extractSentences(response)\n",
    "        word = response[\"Name\"]\n",
    "        if result == \"FAIL\":\n",
    "            print(word + \" has failed in extraction.\") # comment out if annoying \n",
    "            fails.append(word)\n",
    "        else:\n",
    "            ret.update(result)\n",
    "        \n",
    "    return ret, fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# setup: set path var + make folders + set done var\n",
    "base_path = os.path.join(os.getcwd(), '.PickleScrapes')\n",
    "\n",
    "for name in NAMES:\n",
    "    check = os.path.join(base_path, name)\n",
    "    if not os.path.exists(check):\n",
    "        os.mkdir(check)\n",
    "\n",
    "done = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment and adjust accordingly if scraping loop got stopped\n",
    "# done = ['Amis', 'Atayal', 'Bunun',\n",
    "#         'Kanakanavu', 'Kavalan', 'Paiwan',\n",
    "#         'Puyma', 'Rukai', 'Saaroa', \n",
    "#         'Saisiyat', 'Sakizaya', 'Seediq',\n",
    "#         'Thao', 'Tsou']\n",
    "\n",
    "# splittribe = 'ENTER TRIBE NAME HERE'\n",
    "# last_ckpt = CHANGE TO NUMBER OF LARGEST CHECKPOINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Truku\n",
      "\tGetting wordlist...\n",
      "\tDoing API requests...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31348/31348 [13:22:55<00:00,  1.54s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Yami\n",
      "\tGetting wordlist...\n",
      "\tDoing API requests...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6320/6320 [2:40:40<00:00,  1.53s/it]   \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(NAMES)):\n",
    "    scrapeTribe = NAMES[i]\n",
    "\n",
    "    # relevant code for broken api runs\n",
    "    if scrapeTribe in done:\n",
    "        continue\n",
    "    \n",
    "    # get wordlist\n",
    "    print(f\"Processing {scrapeTribe}\")\n",
    "    print(f\"\\tGetting wordlist...\")\n",
    "    words = set(getWords(NAME_TO_IDX[scrapeTribe]))\n",
    "    words = sorted(list(words)) \n",
    "    words = [w for w in words if any(char.isalnum() for char in w)]\n",
    "\n",
    "    # setup vars to save\n",
    "    word_sent_dict = {}\n",
    "    fails = []\n",
    "    seen = set()\n",
    "    tribepath = os.path.join(base_path, scrapeTribe, scrapeTribe)\n",
    "\n",
    "    # api request loop\n",
    "    print(f\"\\tDoing API requests...\")\n",
    "    for i, query in enumerate(tqdm(words)):\n",
    "        try:\n",
    "            response = getData(scrapeTribe, query)\n",
    "\n",
    "            # relevant code for broken api runs\n",
    "            # if scrapeTribe == splittribe:\n",
    "            #     if i <= last_ckpt:\n",
    "            #         continue          \n",
    "\n",
    "            # add to buckets\n",
    "            if response == 'FAIL':\n",
    "                fails.append(query)\n",
    "            word_sent_dict[query] = response\n",
    "\n",
    "            # checkpoint progress\n",
    "            if i % INTERVAL == 0 and i != 0:\n",
    "                with open(tribepath + '_ckpt_{0}.pkl'.format(i), 'wb') as f: \n",
    "                    pickle.dump(word_sent_dict, f)\n",
    "                if fails: \n",
    "                    with open(tribepath + '_fails_{0}.pkl'.format(i), 'wb') as f: \n",
    "                        pickle.dump(fails, f)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    # write final result\n",
    "    with open(tribepath + '_ckpt_END.pkl', 'wb') as f: \n",
    "        pickle.dump(word_sent_dict, f)\n",
    "    with open(tribepath + '_fails_END.pkl', 'wb') as f: \n",
    "        pickle.dump(fails, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31303"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # stitching -- code may be relevant if you had to run in seperate runs\n",
    "# # may be applicable especially to truku\n",
    "\n",
    "# tribepath = '.PickleScrapes\\\\TRIBE_NAME\\\\'\n",
    "\n",
    "# tpic1, tpic2 = None, None\n",
    "# with open(tribepath + 'Truku_ckpt_aside-pt1.pkl', 'rb') as f: \n",
    "#     tpic1 = pickle.load(f)\n",
    "\n",
    "# with open(tribepath + 'Truku_ckpt_aside-pt2.pkl', 'rb') as f: \n",
    "#     tpic2 = pickle.load(f)\n",
    "\n",
    "# clone = tpic1.copy()\n",
    "# clone.update(tpic2)\n",
    "\n",
    "# with open(tribepath + \"Truku_ckpt_END.pkl\", 'wb') as f:\n",
    "#     pickle.dump(clone, f)\n",
    "\n",
    "# print(f\"{len(clone)}\\t{len(tpic1) + len(tpic2)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
